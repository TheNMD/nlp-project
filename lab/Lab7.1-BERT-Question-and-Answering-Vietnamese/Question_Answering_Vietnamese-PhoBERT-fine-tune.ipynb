{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"c5WjNWHM8xPC"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YO6iU2HU82OY"},"outputs":[],"source":["!pip install transformers\n","!pip install datasets"]},{"cell_type":"markdown","metadata":{"id":"VruGpXaeAY5A"},"source":["# 1. Import libs"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8rtcQZMQ9r8Y"},"outputs":[],"source":["import os, sys, argparse, gc\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering, DefaultDataCollator\n","import datasets\n","from typing import Any"]},{"cell_type":"markdown","metadata":{"id":"Jum5gJTe93Hi"},"source":["# 2. Run code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcqppLge953e"},"outputs":[],"source":["print(\"TF Version: \", tf.__version__)\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","    # Currently, memory growth needs to be the same across GPUs\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","    except RuntimeError as e:\n","        # Memory growth must be set before GPUs have been initialized\n","        print(e)\n","\n","#########################################################\n","#MODEL_PHOBERT_BASE = 'vinai/phobert-base'\n","MODEL_PHOBERT_LARGE = 'vinai/phobert-large'\n","\n","#########################################################\n","def get_arguments():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--model\", type=str, default=MODEL_PHOBERT_LARGE, help=\"Pretrained model bert\")\n","    parser.add_argument(\"--lr\", type=float, default=1e-5, help=\"Learning rate\")\n","    parser.add_argument(\"--bs\", type=int, default=16, help=\"Batch size\")\n","    parser.add_argument(\"--epochs\", type=int, default=20, help=\"Number of epochs\")\n","    parser.add_argument(\"--maxlen\", type=int, default=512, help=\"Max sentence length\")\n","    parser.add_argument(\"--stride\", type=int, default=128, help=\"Stride value for window slide\")\n","    parser.add_argument(\"--use_fast\", type=bool, default=True, help=\"Tokenize sentence with fast bpe\")\n","\n","    return parser.parse_args(args=[])\n","\n","#########################################################\n","def preprocess_dataset(ds: pd.DataFrame, tokenizer: Any, maxlen: int):\n","    questions = [q.strip() for q in ds[\"question\"]]\n","    contexts = [str(t) for t in ds[\"context\"]]\n","    inputs = tokenizer(\n","        questions,\n","        contexts,\n","        max_length=maxlen,\n","        truncation=\"only_second\",\n","        return_token_type_ids=True,\n","        padding=\"max_length\",\n","    )\n","\n","    answer_starts = ds[\"answer_start\"]\n","    answer_ends = ds[\"answer_end\"]\n","    answers = ds[\"answer\"]\n","    start_positions = []\n","    end_positions = []\n","    assert len(answer_starts) == len(answer_ends)\n","\n","    for i in range(len(answer_starts)):\n","        start_char = answer_starts[i]\n","        end_char = answer_ends[i]\n","        if start_char == 0 and end_char == 0:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","            continue\n","\n","        answer = answers[i]\n","        context = contexts[i]\n","        input_ids = inputs[\"input_ids\"][i]\n","\n","        # Find the start and end of the context\n","        idx = 0\n","        while input_ids[idx] != 2:\n","            idx += 1\n","        idx += 2\n","        context_start = idx\n","        # print(\"context start:\", context_start, \",id:\", input_ids[context_start])\n","        while idx < len(input_ids) and input_ids[idx] != 1:\n","            idx += 1\n","        context_end = idx - 1\n","        if input_ids[context_end] == 2:\n","            context_end -= 1\n","        # print(\"context end:\", context_end, \"id:\", input_ids[context_end])\n","\n","        pre_ans = tokenizer.encode(context[:start_char], add_special_tokens=False)\n","        ans_ids = tokenizer.encode(answer, add_special_tokens=False)\n","\n","        start_position = context_start + len(pre_ans)\n","        end_position = start_position + len(ans_ids) - 1\n","\n","        # If the answer is not fully inside the context, label is (0, 0)\n","        if start_position < context_start or end_position > context_end:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # Otherwise it's the start and end token positions\n","            start_positions.append(start_position)\n","            end_positions.append(end_position)\n","\n","    inputs[\"start_positions\"] = np.array(start_positions, dtype=np.int32)\n","    inputs[\"end_positions\"] = np.array(end_positions, dtype=np.int32)\n","    return inputs\n","\n","def generate_dataset(file_name: str, tokenizer: Any, data_collator: Any, maxlen: int, stride: int, batch_size: int, model_name: str):\n","    df = pd.read_csv(file_name)\n","    df.drop(\"title\", axis=1, inplace=True)\n","    df.fillna({\"answer\": \"\"}, inplace=True)\n","    ds = datasets.Dataset.from_dict(df)\n","\n","    dataset =  ds.map(\n","       lambda x: preprocess_dataset(x, tokenizer, maxlen),\n","       batched=True,\n","       remove_columns=ds.column_names,\n","    )\n","    \n","    return dataset.to_tf_dataset(\n","        columns=[\n","            \"input_ids\",\n","            \"start_positions\",\n","            \"end_positions\",\n","            \"attention_mask\",\n","            \"token_type_ids\",\n","        ],\n","        collate_fn=data_collator,\n","        shuffle=True,\n","        batch_size=batch_size,\n","    )\n","\n","#########################################################\n","class myCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, saved_model_name: str):\n","        super().__init__()\n","\n","        self.min_loss = sys.float_info.max\n","        self.min_val_loss = sys.float_info.max\n","\n","        self.saved_model_name = saved_model_name\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        min_loss = logs.get('loss')\n","        min_val_loss = logs.get('val_loss')\n","\n","        if min_loss <= self.min_loss and min_val_loss <= self.min_val_loss:\n","            self.min_loss = min_loss\n","            self.min_val_loss = min_val_loss\n","\n","            print(\"\\nsave model at epoch {}\".format(epoch+1))\n","            # self.model.save(\"models/{}.h5\".format(self.saved_model_name))\n","            self.model.save(\"/content/gdrive/MyDrive/Research/NLP-Labs/Lab7.1_BERT-Question-and-Answering-Vietnamese/model/vinai-phobert-large\", save_format='tf')\n","            \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yU_Zb5qLAjeO"},"outputs":[],"source":["if __name__ == \"__main__\":\n","    args = get_arguments()\n","    model_name = args.model\n","    lr = args.lr\n","    batch_size = 2 #args.bs\n","    epochs = 10 # args.epochs\n","    maxlen = args.maxlen\n","    stride = args.stride\n","    use_fast = args.use_fast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xuMN1FzX0s7Z"},"outputs":[],"source":["#########################################################\n","# separate so that later test runs, we don't execute (train) again\n","\n","if __name__ == \"__main__\":    \n","    print(\"##############################\")\n","    print(\"Model :\", model_name)\n","    print(\"Learning Rate :\", lr)\n","    print(\"Batch Size :\", batch_size)\n","    print(\"Epochs :\", epochs)\n","    print(\"Max Token Length :\", maxlen)\n","    print(\"Stride :\", stride)\n","    print(\"Use Fast :\", use_fast)\n","    print(\"##############################\")\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=use_fast)\n","    data_collator = DefaultDataCollator(return_tensors=\"tf\")\n","    dataset_tr = generate_dataset(\n","        \"/content/gdrive/MyDrive/Research/NLP-Labs/Lab7.1_BERT-Question-and-Answering-Vietnamese/datasets/ViWikiQA1.0/ws_train.csv\",\n","        tokenizer, data_collator, maxlen,\n","        stride, batch_size, model_name\n","    )\n","    dataset_val = generate_dataset(\n","        \"/content/gdrive/MyDrive/Research/NLP-Labs/Lab7.1_BERT-Question-and-Answering-Vietnamese/datasets/ViWikiQA1.0/ws_dev.csv\",\n","        tokenizer, data_collator, maxlen, stride,\n","        batch_size, model_name\n","    )\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","\n","    model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)\n","    model.compile(optimizer=optimizer)\n","\n","    # Train in mixed-precision float16\n","    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n","\n","    cb = myCallback(model_name.replace(\"/\", \"-\"))\n","    earlyStopCB = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=3, verbose=1)\n","\n","    history = model.fit(\n","        dataset_tr,\n","        validation_data=dataset_val,\n","        epochs=epochs,\n","        callbacks=[cb, earlyStopCB],\n","    )\n","\n","    hist = pd.DataFrame(history.history)\n","    hist.to_csv(\"{}_bs{}_lr{}.csv\".format(model_name.replace(\"/\", \"-\"), batch_size, lr))"]},{"cell_type":"markdown","metadata":{"id":"NjtyVuMLO10a"},"source":["# 3. Load Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fK_Oj--KPEDL"},"outputs":[],"source":["df_test = pd.read_csv(\"/content/gdrive/MyDrive/Research/NLP-Labs/Lab7.1_BERT-Question-and-Answering-Vietnamese/datasets/ViWikiQA1.0/ws_test.csv\")\n","df_test.drop(\"title\", axis=1, inplace=True)\n","df_test.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZeZOX0ezPmVT"},"outputs":[],"source":["df_test[\"context_len\"] = df_test[\"context\"].apply(lambda x: len(x.split()))\n","df_test[\"context_len\"].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxoeE0GDP4gE"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","ds_test = datasets.Dataset.from_dict(df_test)\n","\n","dataset_test = ds_test.map(\n","    lambda ds: preprocess_dataset(ds, tokenizer, maxlen),\n","    batched=True,\n","    remove_columns=ds_test.column_names,\n",")\n","len(ds_test), len(dataset_test)\n","#dataset_test\n","#dataset_test[0]['end_positions']"]},{"cell_type":"markdown","metadata":{"id":"sSpQvZnCRtAy"},"source":["# 4. Load model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKx2yelbRylR"},"outputs":[],"source":["model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)\n","model.load_weights(f\"/content/gdrive/MyDrive/Research/NLP-Labs/Lab7.1_BERT-Question-and-Answering-Vietnamese/model/vinai-phobert-large\")\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"9PD9dP_6SXfw"},"source":["# 5. Predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrRve7NQSZYR"},"outputs":[],"source":["def tokenize_question_context(question, context, tokenizer, maxlen, stride):\n","    question = question.strip()\n","    context = context.strip()\n","    inputs = tokenizer(\n","        question,\n","        context,\n","        max_length=maxlen,\n","        truncation=\"only_second\",\n","        stride=stride,\n","        # return_overflowing_tokens=True,\n","        # return_offsets_mapping=True,\n","        padding=\"max_length\",\n","        return_tensors=\"tf\"\n","    )\n","\n","    return inputs\n","  \n","def predict(model, question, context, tokenizer, maxlen, stride):\n","    inputs = tokenize_question_context(question, context, tokenizer, maxlen, stride)\n","    outputs = model(**inputs)\n","\n","    start_logits = outputs[\"start_logits\"].numpy()\n","    end_logits = outputs[\"end_logits\"].numpy()\n","\n","    starts = np.argmax(start_logits, axis=1)\n","    ends = np.argmax(end_logits,  axis=1)\n","\n","    start_scores = np.max(start_logits, axis=1)\n","    end_scores = np.max(end_logits, axis=1)\n","    scores = start_scores + end_scores\n","\n","    indices = []\n","    for idx, start in enumerate(starts):\n","        end = ends[idx]\n","        if start == 0 and end == 0:\n","            continue\n","        if end < start:\n","            continue\n","        indices.append(idx)\n","\n","    answers = []\n","    for idx in indices:\n","        score = scores[idx]\n","        ans_ids = inputs[\"input_ids\"][idx][starts[idx]:ends[idx]+1]\n","        answer = tokenizer.decode(ans_ids, skip_special_tokens=True)\n","        answers.append((answer, score))\n","    return answers\n","\n","\n","df = df_test[df_test[\"context_len\"] > 500]\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVFcYd08Sndp"},"outputs":[],"source":["df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNCNzpmhStpR"},"outputs":[],"source":["idx = 4\n","# question = df.loc[idx, \"question\"]\n","# context = df.loc[idx, \"context\"]\n","question = df_test.loc[idx, \"question\"]\n","print(\"Question:\", question)\n","context = df_test.loc[idx, \"context\"]\n","print(\"Context:\", context)\n","\n","answers = predict(model, question, context, tokenizer, maxlen, stride)\n","print(\"Answer:\",answers)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPEDNxNzF0bkxms+1crucQJ","machine_shape":"hm","mount_file_id":"1uKf8pNtbMRMYCrysu8Dq5843BUSIxHVU","private_outputs":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
